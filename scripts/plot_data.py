#!/usr/bin/env python3
import argparse
import csv
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns


def plot_entropy(df):
    # get current axis
    axes = plt.gca()
    df.plot(kind='line', x='frame', y='Pose_entropy', ax=axes)
    mean = np.nanmean(df.Pose_entropy)
    plt.axhline(y=mean, color='red', linestyle='--', label='mean')
    var = np.nanvar(df.Pose_entropy)
    std = np.sqrt(var)
    print("--------------")
    print("Mean: ", mean)
    print("Variance: ", var)
    print("Std: ", std)
    print("--------------")
    plt.fill_between(axes.get_xlim(),mean+std,mean-std, color='red', alpha=.5)
    plt.xlabel('Frames')
    plt.ylabel('Pose entropy')
    plt.show()

def compute_pose_correlation(df, error_array):
    entropy_array = df.Pose_entropy.to_numpy()
    correlation = np.corrcoef(entropy_array, error_array)
    print("Correlation: ", correlation)
    plt.scatter(entropy_array, error_array)
    plt.title('RPE and entropy correlation')
    plt.xlabel('Entropy')
    plt.ylabel('RPE')
    plt.show()

def concatenate_data(df, error_array, column_name, output):
    if 'mean_time' in df.columns:
        df.drop(columns=['mean_time'], axis=1, inplace=True)
    
    if column_name not in df.columns:
        df.dropna(how='all', axis=1, inplace=True)
        df[column_name] = error_array.tolist()
        df.to_csv(output, encoding='utf-8', index=False)

def compute_data_correlation(df, output):
    if 'frame' in df.columns:
        df.drop(columns=['frame'], axis=1, inplace=True)
    corrM = df.corr()
    print(corrM)
    fig, ax = plt.subplots(figsize=(15,15))
    sns.heatmap(corrM, annot=True, fmt='.4f', 
    cmap=plt.get_cmap('coolwarm'), cbar=False, ax=ax)
    ax.set_yticklabels(ax.get_yticklabels(), rotation="horizontal")
    plt.savefig(output, bbox_inches='tight', pad_inches=0.0)

def plot_data(df, fields, range_values, normalize_data):
    vectors = {}
    for field in fields:
        data = []
        for index in range(range_values[0], range_values[1]+1):
            values = df.loc[df['frame']==index, field].values
            data.append(values[0])
        vectors[field] = data
    
    fig, ax = plt.subplots()
    x = list(range(range_values[0], range_values[1]+1))
    for key, values in vectors.items():
        if normalize_data:
            values_norm = (values - np.min(values)) / (np.max(values) - np.min(values))
            ax.plot(x, values_norm, label=key)
        else:
            ax.plot(x, values, label=key)
    
    ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))
    plt.legend()
    plt.show()

parser = argparse.ArgumentParser(
    description='Plot data from csv file generated by the KLTVO library.')

parser.add_argument('--file', '-f',
                    help='CSV file with the data provided by kltvo')

parser.add_argument('--error_file',
                    help='File containing the vector of errors computed by evo')

parser.add_argument('--plot_entropy',
                    action="store_true",
                    help='Plot pose entropy')

parser.add_argument('--compute_pose_correlation',
                    action="store_true",
                    help='Compute correlation between provided pose entropy and provided error array')

parser.add_argument('--compute_df_correlation',
                    action="store_true",
                    help='Compute correlation between all data')

parser.add_argument('--output', '-o',
                    help='Output directory')

parser.add_argument('--fuse_data',
                    action="store_true",
                    help='Fuse np array with worksheet')

parser.add_argument('--column_name',
                    help='name of the new column to be created')

parser.add_argument('--plot_multi_data',
                    action="store_true",
                    help='Plot multiple defined data in the same graph')

parser.add_argument('--plot_fields',
                    nargs='+',
                    help='List of the fields to be analyzed')
parser.add_argument('--plot_range',
                    nargs='+',
                    type=int,
                    help='Range of data to be analyzed (e.g. frame index)')
parser.add_argument('--normalize_data',
                    action="store_true",
                    help='Set this argument if you want to normalize the data between 0 and 1')

args = parser.parse_args()

if args.file is not None:
    fh = open(args.file)

    csv_reader = csv.reader(fh)
    csv_header = next(csv_reader)
    fh.close()
    df = pd.read_csv(args.file, header=None, skiprows=1, names=csv_header)

if args.error_file is not None:
    error_array = np.load(args.error_file)

if(args.plot_entropy):
    plot_entropy(df)

if(args.compute_pose_correlation):
    compute_pose_correlation(df,error_array, args.all)

if(args.fuse_data):
    if args.output is not None:
        concatenate_data(df, error_array, args.column_name, args.output)
    else:
        concatenate_data(df, error_array, args.column_name, args.file)

if(args.compute_df_correlation):
    compute_data_correlation(df, args.output)

if(args.plot_multi_data):
    plot_data(df, args.plot_fields, args.plot_range, args.normalize_data)